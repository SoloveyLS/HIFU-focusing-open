import torch
from torch import nn
from torch.nn import functional as F

# fit with matrix of N_rays_to_fit x L_ray
# forward: N_rays x L_ray -> N_rays x components_number
# coef2series: N_rays x components_number -> N_rays x L_ray 
class KarhunenLoeveTransform(nn.Module):
    def __init__(
        self, 
        component_number=64,
        return_error=False,
        upscale=1
    ):
        super().__init__()
        self.K = torch.tensor([])
        self.eigvec = torch.tensor([])
        self.eigval = torch.tensor([])
        self.basis_mean = torch.tensor([])
        self.component_number = component_number
        self.return_error = return_error
        
    def fit(self, data):
    # data: Rays * Ray_length
        self.K = torch.zeros(data.shape[1], data.shape[1])
        self.basis_mean = data.mean(dim=0)
        
        for indx in range(data.shape[1]):
            for indy in range(data.shape[1]):
                self.K[indx, indy] = \
                            ((data[:, indx] - self.basis_mean[indx])* \
                             (data[:, indy] - self.basis_mean[indy])).sum()
        self.eigval, self.eigvec = torch.linalg.eig(self.K)
        # if self.eigvec.imag.abs().max() < 1e-5:
        self.eigvec = self.eigvec.real

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)
        ray_num = x.shape[0]
        series_coefs = torch.matmul((x - self.basis_mean.expand_as(x).to(x.device)).unsqueeze(1), self.eigvec[:, :self.component_number].unsqueeze(0).to(x.device))
        return series_coefs.squeeze()

    def coefs2series(self, x, signal_to_compare=torch.Tensor([])):
        device = x.device
        
        predicted = torch.zeros(x.shape[0], self.basis_mean.shape[0]).to(device)
        if signal_to_compare.numel():
            MSE = torch.zeros(x.shape[0], min([self.component_number, x.shape[0]])).to(device)
            MAE = torch.zeros(x.shape[0], min([self.component_number, x.shape[0]])).to(device)
            
        for ind1 in range(x.shape[0]):
            if self.return_error:
                predicted_iter = self.basis_mean
                
                for ind2 in range(min([self.component_number, x.shape[0]])):
                    predicted_iter = predicted_iter.to(device) + x[ind1, ind2] * self.eigvec[:,ind2].to(device)
                    
                    if signal_to_compare.numel():
                        MAE[ind1, ind2] = torch.nn.functional.l1_loss (predicted_iter.to(device), signal_to_compare[ind1].to(device))
                        MSE[ind1, ind2] = torch.nn.functional.mse_loss(predicted_iter.to(device), signal_to_compare[ind1].to(device))
                predicted[ind1, :] = predicted_iter
                
        if signal_to_compare.numel():
            return predicted, MAE.cpu(), MSE.cpu()
        return predicted


# TEST DATA:
# data = torch.tensor((
#    (1.4631000e+03,   1.4511000e+03,   1.4602000e+03,   1.4540000e+03,   1.4557000e+03,   1.4443000e+03,   1.4644000e+03,   1.4503000e+03,   1.4636000e+03,   1.4510000e+03,   1.4512000e+03,   1.4426000e+03,   1.4646000e+03,   1.4524000e+03,   1.4618000e+03,   1.4599000e+03,   1.4457000e+03,   1.4460000e+03,   1.4452000e+03,   1.4616000e+03,   1.4463000e+03,   1.4582000e+03,   1.4580000e+03,   1.4632000e+03,   1.4457000e+03,   1.4527000e+03,   1.4506000e+03,   1.4649000e+03,   1.4652000e+03,   1.4501000e+03),
#    (1.4635000e+03,   1.4513000e+03,   1.4605000e+03,   1.4542000e+03,   1.4559000e+03,   1.4445000e+03,   1.4648000e+03,   1.4504000e+03,   1.4639000e+03,   1.4512000e+03,   1.4514000e+03,   1.4428000e+03,   1.4650000e+03,   1.4526000e+03,   1.4619000e+03,   1.4601000e+03,   1.4458000e+03,   1.4462000e+03,   1.4453000e+03,   1.4618000e+03,   1.4465000e+03,   1.4588000e+03,   1.4582000e+03,   1.4634000e+03,   1.4460000e+03,   1.4529000e+03,   1.4508000e+03,   1.4651000e+03,   1.4654000e+03,   1.4502000e+03),
#    (1.4640000e+03,   1.4515000e+03,   1.4607000e+03,   1.4543000e+03,   1.4560000e+03,   1.4446000e+03,   1.4650000e+03,   1.4505000e+03,   1.4642000e+03,   1.4514000e+03,   1.4516000e+03,   1.4431000e+03,   1.4654000e+03,   1.4528000e+03,   1.4621000e+03,   1.4602000e+03,   1.4460000e+03,   1.4463000e+03,   1.4455000e+03,   1.4620000e+03,   1.4466000e+03,   1.4591000e+03,   1.4583000e+03,   1.4635000e+03,   1.4463000e+03,   1.4530000e+03,   1.4509000e+03,   1.4653000e+03,   1.4655000e+03,   1.4503000e+03),
#    (1.4644000e+03,   1.4516000e+03,   1.4610000e+03,   1.4545000e+03,   1.4562000e+03,   1.4448000e+03,   1.4651000e+03,   1.4507000e+03,   1.4646000e+03,   1.4516000e+03,   1.4517000e+03,   1.4432000e+03,   1.4656000e+03,   1.4530000e+03,   1.4626000e+03,   1.4604000e+03,   1.4462000e+03,   1.4465000e+03,   1.4457000e+03,   1.4621000e+03,   1.4468000e+03,   1.4593000e+03,   1.4585000e+03,   1.4637000e+03,   1.4465000e+03,   1.4532000e+03,   1.4511000e+03,   1.4655000e+03,   1.4657000e+03,   1.4504000e+03),
#    (1.4652000e+03,   1.4520000e+03,   1.4615000e+03,   1.4548000e+03,   1.4565000e+03,   1.4452000e+03,   1.4655000e+03,   1.4510000e+03,   1.4653000e+03,   1.4519000e+03,   1.4520000e+03,   1.4434000e+03,   1.4659000e+03,   1.4533000e+03,   1.4635000e+03,   1.4607000e+03,   1.4465000e+03,   1.4468000e+03,   1.4460000e+03,   1.4622000e+03,   1.4471000e+03,   1.4599000e+03,   1.4588000e+03,   1.4640000e+03,   1.4471000e+03,   1.4535000e+03,   1.4514000e+03,   1.4659000e+03,   1.4660000e+03,   1.4506000e+03),
#    (1.4659000e+03,   1.4528000e+03,   1.4619000e+03,   1.4550000e+03,   1.4569000e+03,   1.4456000e+03,   1.4661000e+03,   1.4514000e+03,   1.4664000e+03,   1.4521000e+03,   1.4524000e+03,   1.4438000e+03,   1.4659000e+03,   1.4539000e+03,   1.4639000e+03,   1.4609000e+03,   1.4469000e+03,   1.4473000e+03,   1.4464000e+03,   1.4626000e+03,   1.4475000e+03,   1.4608000e+03,   1.4596000e+03,   1.4643000e+03,   1.4476000e+03,   1.4539000e+03,   1.4521000e+03,   1.4669000e+03,   1.4673000e+03,   1.4523000e+03),
#    (1.4666000e+03,   1.4535000e+03,   1.4623000e+03,   1.4552000e+03,   1.4573000e+03,   1.4460000e+03,   1.4667000e+03,   1.4518000e+03,   1.4675000e+03,   1.4523000e+03,   1.4528000e+03,   1.4442000e+03,   1.4660000e+03,   1.4546000e+03,   1.4643000e+03,   1.4611000e+03,   1.4473000e+03,   1.4477000e+03,   1.4469000e+03,   1.4629000e+03,   1.4478000e+03,   1.4611000e+03,   1.4603000e+03,   1.4645000e+03,   1.4480000e+03,   1.4544000e+03,   1.4528000e+03,   1.4680000e+03,   1.4686000e+03,   1.4539000e+03),
#    (1.4674000e+03,   1.4544000e+03,   1.4625000e+03,   1.4547000e+03,   1.4555000e+03,   1.4468000e+03,   1.4680000e+03,   1.4527000e+03,   1.4683000e+03,   1.4525000e+03,   1.4529000e+03,   1.4451000e+03,   1.4664000e+03,   1.4554000e+03,   1.4662000e+03,   1.4606000e+03,   1.4482000e+03,   1.4486000e+03,   1.4479000e+03,   1.4635000e+03,   1.4485000e+03,   1.4643000e+03,   1.4598000e+03,   1.4653000e+03,   1.4488000e+03,   1.4534000e+03,   1.4537000e+03,   1.4688000e+03,   1.4695000e+03,   1.4547000e+03),
#    (1.4683000e+03,   1.4553000e+03,   1.4593000e+03,   1.4538000e+03,   1.4513000e+03,   1.4477000e+03,   1.4692000e+03,   1.4536000e+03,   1.4690000e+03,   1.4494000e+03,   1.4469000e+03,   1.4459000e+03,   1.4670000e+03,   1.4563000e+03,   1.4676000e+03,   1.4601000e+03,   1.4468000e+03,   1.4476000e+03,   1.4483000e+03,   1.4641000e+03,   1.4454000e+03,   1.4651000e+03,   1.4562000e+03,   1.4661000e+03,   1.4501000e+03,   1.4511000e+03,   1.4545000e+03,   1.4697000e+03,   1.4687000e+03,   1.4556000e+03),
#    (1.4691000e+03,   1.4561000e+03,   1.4547000e+03,   1.4534000e+03,   1.4518000e+03,   1.4484000e+03,   1.4692000e+03,   1.4544000e+03,   1.4702000e+03,   1.4474000e+03,   1.4452000e+03,   1.4468000e+03,   1.4669000e+03,   1.4572000e+03,   1.4694000e+03,   1.4597000e+03,   1.4463000e+03,   1.4471000e+03,   1.4486000e+03,   1.4632000e+03,   1.4463000e+03,   1.4660000e+03,   1.4552000e+03,   1.4665000e+03,   1.4514000e+03,   1.4515000e+03,   1.4554000e+03,   1.4705000e+03,   1.4678000e+03,   1.4564000e+03),
#    (1.4700000e+03,   1.4570000e+03,   1.4556000e+03,   1.4543000e+03,   1.4522000e+03,   1.4492000e+03,   1.4684000e+03,   1.4553000e+03,   1.4711000e+03,   1.4483000e+03,   1.4461000e+03,   1.4477000e+03,   1.4666000e+03,   1.4580000e+03,   1.4703000e+03,   1.4606000e+03,   1.4471000e+03,   1.4462000e+03,   1.4495000e+03,   1.4617000e+03,   1.4471000e+03,   1.4668000e+03,   1.4519000e+03,   1.4652000e+03,   1.4523000e+03,   1.4524000e+03,   1.4563000e+03,   1.4709000e+03,   1.4682000e+03,   1.4573000e+03),
#    (1.4717000e+03,   1.4588000e+03,   1.4573000e+03,   1.4560000e+03,   1.4539000e+03,   1.4509000e+03,   1.4702000e+03,   1.4571000e+03,   1.4728000e+03,   1.4500000e+03,   1.4478000e+03,   1.4494000e+03,   1.4683000e+03,   1.4598000e+03,   1.4720000e+03,   1.4623000e+03,   1.4488000e+03,   1.4479000e+03,   1.4512000e+03,   1.4634000e+03,   1.4488000e+03,   1.4686000e+03,   1.4537000e+03,   1.4669000e+03,   1.4540000e+03,   1.4542000e+03,   1.4580000e+03,   1.4727000e+03,   1.4699000e+03,   1.4591000e+03),
# ))
# data = data.permute((1,0))